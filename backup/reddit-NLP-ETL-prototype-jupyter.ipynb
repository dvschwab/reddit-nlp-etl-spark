{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjlGoLncKYVm"
   },
   "source": [
    "# reddit NLP ETL prototype\n",
    "\n",
    "This code prototypes a Spark workflow that retrieves data sets of compressed reddit posts and outputs TF-IDF scores for each post. After processing, a sample of records is exported to Amazon's DynamoDB for further analysis by a data science team.\n",
    "\n",
    "This code has been tested on Google Colab's Spark engine for the years 2006 and 2007. A more thorough test is planned using a five-node Spark cluster provisioned from Amazon's EMR service. The entire workflow, spanning 10 years of posts and approximately 2.5B records, will be run on a 12-node EMR cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1sd9SzuUvob",
    "outputId": "85d0d21e-6a4f-4595-b885-25227dbf20b2"
   },
   "outputs": [],
   "source": [
    "# Install Spark and its dependencies and configure environment\n",
    "\n",
    "import os\n",
    "\n",
    "spark_version = 'spark-3.0.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuRwcXyoSnqS"
   },
   "source": [
    "## Initialize Spark and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnAKMFhgST-r"
   },
   "outputs": [],
   "source": [
    "# Initialize Spark instance\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "su95fH1IYSqt"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# File handling and NLP\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "\n",
    "# requests used to retrieve files\n",
    "# bz2 to unzip them (bzip2 compression)\n",
    "# time calculates running time of main loop\n",
    "\n",
    "import requests\n",
    "import bz2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRmMt8FVirP"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RedditNlpEtlPoCTest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tlx_d-6P7I6-"
   },
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L94KiWkd7NMa"
   },
   "source": [
    "### Data Retrieval and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DG53pHdQ3uBv"
   },
   "outputs": [],
   "source": [
    "def generate_urls(url_stub, start_year, end_year):\n",
    "  \"\"\"\n",
    "  Generates a list of URLs corresponding the the Reddit posts\n",
    "  to be retrieved. The URL pattern is somewhat idiosyncratic,\n",
    "  so if it changes this code will need to be modified.\n",
    "\n",
    "  Arguments:\n",
    "    url_stub: the common prefix used for all URLs\n",
    "    start_year, end_year: the beginning and ending years\n",
    "    of posts to retrieve. For a single year, set\n",
    "    start_year = end_year. Both start_year and end_year\n",
    "    are inclusive, so (2010, 2012) will return three years\n",
    "    of URLs\n",
    "\n",
    "  Returns:\n",
    "    list of URLs with one URL for each month and year\n",
    "    to be retrieved\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  # URLs appended to this list\n",
    "  file_urls = []\n",
    "  \n",
    "  for year in range(start_year, (end_year + 1)):\n",
    "    \n",
    "    # Set extension by year for most files\n",
    "      \n",
    "    if year < 2018:\n",
    "      extension = '.bz2'\n",
    "    elif year < 2019:\n",
    "      extension = '.xz'\n",
    "    else:\n",
    "      extension = '.zst'\n",
    "      \n",
    "    # Generate the URLs based on the observed patterns\n",
    "\n",
    "    for month in range(1,13):\n",
    "          \n",
    "      # Handle a few special cases    \n",
    "      if (year == 2017 and month == 12):\n",
    "        extension = '.xz'\n",
    "      if (year == 2018 and month in [11,12]):\n",
    "        extension = '.zst'\n",
    "\n",
    "      # Create the file name, adding the leading zero\n",
    "      # if the month is 1 - 9\n",
    "      if month < 10:\n",
    "        file = 'RC_' + str(year) + '-0' + str(month) + extension\n",
    "      else:\n",
    "        file = 'RC_' + str(year) + '-' + str(month) + extension\n",
    "              \n",
    "      file_urls.append(url_stub + file)\n",
    "\n",
    "  return file_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ya8uwHUA_68"
   },
   "outputs": [],
   "source": [
    "def get_file(url):\n",
    "  \"\"\"\n",
    "    Retrieves a file from the supplied URL using the requests library.\n",
    "    Basically a wrapper for requests.get(), but abstracted as a function\n",
    "    so type-checking and error-handling can be added if needed.\n",
    "\n",
    "    Argument:\n",
    "      url: the URL specifying the resource to be retrieved\n",
    "\n",
    "    Returns:\n",
    "      The content of the request response. In this invocation, this\n",
    "      will be the compressed file of reddit posts at the supplied URL\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  response = requests.get(url)\n",
    "\n",
    "  return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cchfPcRhBa1W"
   },
   "outputs": [],
   "source": [
    "def unzip_file(zipped_file):\n",
    "  \"\"\"\n",
    "    Decompresses the supplied file, which must be in bzip2 format.\n",
    "    Requires the bz2 library.\n",
    "\n",
    "    Argument:\n",
    "      The compressed file in bzip2 format\n",
    "\n",
    "    Returns:\n",
    "      A (potentially very long) string containing the uncompressed\n",
    "      file contents.\n",
    "  \"\"\"\n",
    "\n",
    "  unzipped_file = bz2.decompress(zipped_file).decode()\n",
    "\n",
    "  return unzipped_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URyVd0hq7W50"
   },
   "source": [
    "### NLP Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez9O4I8f17gG"
   },
   "outputs": [],
   "source": [
    "def tokenize(nlp_df):\n",
    "    \n",
    "  nlp_tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"post_words\")\n",
    "  tokenized_df = nlp_tokenizer.transform(nlp_df)\n",
    "  return tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRJGCaX02fP2"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_df):\n",
    "\n",
    "  remover = StopWordsRemover(inputCol='post_words', outputCol='post_filtered')\n",
    "  remover.loadDefaultStopWords('english')\n",
    "  filtered_df = remover.transform(tokenized_df)\n",
    "  return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57jSXMLz3ESU"
   },
   "outputs": [],
   "source": [
    "def hasher(hashable_df):\n",
    "\n",
    "# Number of Features is default (262,144)\n",
    "\n",
    "  hasher = HashingTF(inputCol='post_filtered', outputCol='post_hashed')\n",
    "  hashed_df = hasher.transform(hashable_df)\n",
    "  return hashed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPzyjjD13sR6"
   },
   "outputs": [],
   "source": [
    "def tfidf_calc(hashed_df):\n",
    "\n",
    "  tfidf = IDF(inputCol='post_hashed', outputCol='post_tfidf')\n",
    "  tfidfModel = tfidf.fit(hashed_df)\n",
    "  tfidf_df = tfidfModel.transform(hashed_df)\n",
    "  return tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96Is8G-X7cwt"
   },
   "source": [
    "## Main Function\n",
    "\n",
    "This function retrieves the reddit posts, decompresses them, and constructs a Spark DataFrame from them. The DataFrame is then processed by the NLP pipeline to output estimated TF-IDF scores for each post.\n",
    "\n",
    "The corpus for this pipeline is the dowloaded posts: if posts from 2006 are downloaded, then the TF-IDF scores will be relative to 2006 posts. If 2006 and 2007 are downloaded, the scores will be relative to both years; and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fohyO5yA5aMp"
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  # Main function for prototype.\n",
    "  # Generates a list of URLs for the provided url_stub and range of years.\n",
    "  # Then uses a loop to retrieve each file, decompress it,\n",
    "  # and generate an RDD.\n",
    "  # The union of these RDDs generates the DataFrame,\n",
    "  # and the NLP pipeline outputs TF-IDF scores for each post\n",
    "  # to a new column.\n",
    "\n",
    "  start_year = 2006\n",
    "  end_year = 2006\n",
    "  url_stub = 'https://files.pushshift.io/reddit/comments/'\n",
    "  reddit_urls = generate_urls(url_stub, start_year, end_year)\n",
    "\n",
    "  sc = spark.sparkContext\n",
    "  #s3 = boto3.resource('s3')\n",
    "\n",
    "  rdd_list = []\n",
    "\n",
    "  # Display starting time (current time)\n",
    "  print(f'Main loop starting at', {time.asctime()}, '\\n')\n",
    "  for url in reddit_urls:\n",
    "\n",
    "    # Get file from website and decompress it to text  \n",
    "    file_name = url.split('/')[5].split('.')[0]\n",
    "    print(f'Now processing', file_name)\n",
    "\n",
    "    reddit_file = get_file(url)\n",
    "    unzipped_file = unzip_file(reddit_file)\n",
    "\n",
    "    # Make an RDD from the file and append it to a list\n",
    "    print('Generating RDD from file and appending to rdd_list\\n')\n",
    "    rdd_list.append(sc.parallelize(unzipped_file.splitlines()))\n",
    "\n",
    "  \n",
    "  # Create Spark DataFrame from RDDs\n",
    "  print(f'Creating DataFrame of posts from years', start_year, 'to', end_year, '\\n')\n",
    "  reddit_df = spark.read.json(sc.union(rdd_list))\n",
    "\n",
    "  # NLP pipeline\n",
    "  print('Beginning NLP pipeline')\n",
    "  print(f'\\t* Tokenizing posts')\n",
    "  reddit_tokenized_df = tokenize(reddit_df)\n",
    "  print(f'\\t* Removing stop words')\n",
    "  reddit_filtered_df = remove_stopwords(reddit_tokenized_df)\n",
    "  print(f'\\t* Hashing posts')\n",
    "  reddit_hashed_df = hasher(reddit_filtered_df)\n",
    "  print(f'\\t* Calculating TF-IDF scores')\n",
    "  reddit_tfidf_df = tfidf_calc(reddit_hashed_df)\n",
    "\n",
    "  # Display concluding time\n",
    "  print(f'\\nScript concluded at', {time.asctime()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxCWVnM5P7sw",
    "outputId": "fe7384c4-5e3f-44f0-a0f6-bb8861add27b"
   },
   "outputs": [],
   "source": [
    "# Check a few rows to make sure everything worked\n",
    "\n",
    "cols = ['author', 'body', 'post_words', 'post_filtered', 'post_hashed', 'post_tfidf']\n",
    "reddit_tfidf_df.select(cols).show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHH8_TqOCE8J",
    "outputId": "39ee95b3-8ba7-45de-c574-6fa65660bee5"
   },
   "outputs": [],
   "source": [
    "# Print schema\n",
    "\n",
    "reddit_tfidf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgcRiIgTDFaR"
   },
   "outputs": [],
   "source": [
    "# Drop unneeded fields\n",
    "\n",
    "reddit_export_df = reddit_tfidf_df.drop(\n",
    "    'author_flair_css_class', 'author_flair_text', 'body', 'created_utc', 'edited', 'link_id', 'parent_id', 'retrieved_on', 'subreddit_id', 'post_words', 'post_filtered', 'post_hashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYj0JCdlEK5S",
    "outputId": "b0f7fc6e-03f1-473e-b470-6bc62b609ced"
   },
   "outputs": [],
   "source": [
    "reddit_export_df.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "get-reddit-files.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
    "theme": {
     "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     },
     "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  },
  "nteract": {
   "version": "0.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
