{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# record-count-estimate\n",
    "\n",
    "A short, ad hoc script to estimate the total number of posts from a small sample using the CLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the number of reddit posts in all files\n",
    "# from a small sample\n",
    "\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "This loads the file with the post counts for each year. Then, it creates an array for the estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file has the post counts from one file for each year\n",
    "# It was generated with grep\n",
    "\n",
    "reddit_file = 'Reddit_Posts/zipped/reddit-post-counts.txt'\n",
    "\n",
    "# Need to split on newline to get rid of it\n",
    "\n",
    "with open(reddit_file) as f:\n",
    "    contents = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RC_2006-12:61018',\n",
       " 'RC_2007-11:372983',\n",
       " 'RC_2008-10:789874',\n",
       " 'RC_2009-09:2032276',\n",
       " 'RC_2010-08:4247982',\n",
       " 'RC_2011-07:10557466',\n",
       " 'RC_2012-06:21897913',\n",
       " 'RC_2013-05:33126225',\n",
       " 'RC_2014-04:42440735',\n",
       " 'RC_2015-04:55005780']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   61018.,   372983.,   789874.,  2032276.,  4247982., 10557466.,\n",
       "       21897913., 33126225., 42440735., 55005780.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make an array with just the actual counts\n",
    "# You can change num_years to see how that affects the sample estimate\n",
    "\n",
    "num_years = 10\n",
    "\n",
    "counts = [line.split(':')[1] for line in contents[0:num_years]]\n",
    "count_array = np.array(counts).astype(float)\n",
    "count_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "Estimate the expected value (mean) and its corresponding standard error, then compute the upper bound of a 95% confidence interval and report the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of posts for all reddit files 2.05e+09\n",
      "Expected standard error 2.09e+08\n",
      "The upper CI is 2.46e+09\n"
     ]
    }
   ],
   "source": [
    "# Estimate the number of posts in all files using the CLT\n",
    "\n",
    "# Number of files = months * years\n",
    "# alpha sets the confidence level to 95%\n",
    "\n",
    "months = 12\n",
    "sample_size = len(count_array) * months\n",
    "alpha = 1.96\n",
    "\n",
    "# Estimate the expected value (mean), std_error,\n",
    "# and the upper CI\n",
    "\n",
    "exp_value = np.mean(count_array) * sample_size\n",
    "std_error = np.std(count_array) * np.sqrt(sample_size)\n",
    "upper_ci = exp_value + (alpha * std_error)\n",
    "\n",
    "print(f\"Expected number of posts for all reddit files {exp_value:0.3}\")\n",
    "print(f\"Expected standard error {std_error:0.3}\")\n",
    "print(f\"The upper CI is {upper_ci:0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "So, with 10 years of posts from 2006 - 2015, the expected number of posts is 2.05B with a standard error of 209M. The upper confidence interval is 2.46B. So, at most the program will need to process 2.46B records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit-nlp-env",
   "language": "python",
   "name": "reddit-nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
