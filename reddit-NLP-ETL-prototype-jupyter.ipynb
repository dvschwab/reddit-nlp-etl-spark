{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "get-reddit-files.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "nbpresent": {
      "slides": {},
      "themes": {
        "default": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
        "theme": {
          "0535adbc-b74f-46cc-9cd6-4eabe2477c8e": {
            "backgrounds": {
              "backgroundColor": {
                "background-color": "backgroundColor",
                "id": "backgroundColor"
              }
            },
            "id": "0535adbc-b74f-46cc-9cd6-4eabe2477c8e",
            "palette": {
              "backgroundColor": {
                "id": "backgroundColor",
                "rgb": [
                  43,
                  43,
                  43
                ]
              },
              "headingColor": {
                "id": "headingColor",
                "rgb": [
                  238,
                  238,
                  238
                ]
              },
              "linkColor": {
                "id": "linkColor",
                "rgb": [
                  19,
                  218,
                  236
                ]
              },
              "mainColor": {
                "id": "mainColor",
                "rgb": [
                  238,
                  238,
                  238
                ]
              }
            },
            "rules": {
              "a": {
                "color": "linkColor"
              },
              "h1": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 7
              },
              "h2": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 5
              },
              "h3": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 3.75
              },
              "h4": {
                "color": "headingColor",
                "font-family": "Oswald",
                "font-size": 3
              },
              "h5": {
                "color": "headingColor",
                "font-family": "Oswald"
              },
              "h6": {
                "color": "headingColor",
                "font-family": "Oswald"
              },
              "h7": {
                "color": "headingColor",
                "font-family": "Oswald"
              },
              "li": {
                "color": "mainColor",
                "font-family": "Lato",
                "font-size": 5
              },
              "p": {
                "color": "mainColor",
                "font-family": "Lato",
                "font-size": 5
              }
            },
            "text-base": {
              "color": "mainColor",
              "font-family": "Lato",
              "font-size": 5
            }
          },
          "cc59980f-cb69-400a-b63a-1fb85ca73c8a": {
            "backgrounds": {
              "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
                "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
              }
            },
            "id": "cc59980f-cb69-400a-b63a-1fb85ca73c8a",
            "palette": {
              "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
                "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "rgb": [
                  252,
                  252,
                  252
                ]
              },
              "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
                "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "rgb": [
                  68,
                  68,
                  68
                ]
              },
              "50f92c45-a630-455b-aec3-788680ec7410": {
                "id": "50f92c45-a630-455b-aec3-788680ec7410",
                "rgb": [
                  197,
                  226,
                  245
                ]
              },
              "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
                "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "rgb": [
                  43,
                  126,
                  184
                ]
              },
              "efa7f048-9acb-414c-8b04-a26811511a21": {
                "id": "efa7f048-9acb-414c-8b04-a26811511a21",
                "rgb": [
                  25.118061674008803,
                  73.60176211453744,
                  107.4819383259912
                ]
              }
            },
            "rules": {
              "a": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
              },
              "blockquote": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-size": 3
              },
              "code": {
                "font-family": "Anonymous Pro"
              },
              "h1": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "font-family": "Merriweather",
                "font-size": 8
              },
              "h2": {
                "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "font-family": "Merriweather",
                "font-size": 6
              },
              "h3": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-family": "Lato",
                "font-size": 5.5
              },
              "h4": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 5
              },
              "h5": {
                "font-family": "Lato"
              },
              "h6": {
                "font-family": "Lato"
              },
              "h7": {
                "font-family": "Lato"
              },
              "li": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-size": 3.25
              },
              "pre": {
                "font-family": "Anonymous Pro",
                "font-size": 4
              }
            },
            "text-base": {
              "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
              "font-family": "Lato",
              "font-size": 4
            }
          }
        }
      }
    },
    "nteract": {
      "version": "0.10.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjlGoLncKYVm"
      },
      "source": [
        "# reddit NLP ETL prototype\n",
        "\n",
        "This code prototypes a Spark workflow that retrieves data sets of compressed reddit posts and outputs TF-IDF scores for each post. After processing, a sample of records is exported to Amazon's DynamoDB for further analysis by a data science team.\n",
        "\n",
        "This code has been tested on Google Colab's Spark engine for the years 2006 and 2007. A more thorough test is planned using a five-node Spark cluster provisioned from Amazon's EMR service. The entire workflow, spanning 10 years of posts and approximately 2.5B records, will be run on a 12-node EMR cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1sd9SzuUvob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d0d21e-6a4f-4595-b885-25227dbf20b2"
      },
      "source": [
        "# Install Spark and its dependencies and configure environment\n",
        "\n",
        "import os\n",
        "\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r                                                                               \r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI3_zaYCRpOS"
      },
      "source": [
        "# Set location of AWS credentials (different from default: this is for boto3)\n",
        "# No longer needed, as files are not saved to S3\n",
        "\n",
        "# os.environ[\"AWS_SHARED_CREDENTIALS_FILE\"] = \"/content/credentials\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0IFrY-vR-X8"
      },
      "source": [
        "# Install application-specific dependencies\n",
        "# Specific versions are required to resolve dependencies associated\n",
        "# with boto3 (and *its* dependencies)\n",
        "# Note the order must be preserved, as the boto3 install will\n",
        "# also upgrade urllib3 to an incompatable version\n",
        "# Also no longer needed\n",
        "\n",
        "#!pip install folium==0.2.1\n",
        "#!pip install requests==2.23.0\n",
        "#!pip install boto3\n",
        "#!pip install urllib3==1.25.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuRwcXyoSnqS"
      },
      "source": [
        "## Initialize Spark and Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnAKMFhgST-r"
      },
      "source": [
        "# Initialize Spark instance\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su95fH1IYSqt"
      },
      "source": [
        "# Imports\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# File handling and NLP\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
        "\n",
        "# requests used to retrieve files\n",
        "# bz2 to unzip them (bzip2 compression)\n",
        "# time calculates running time of main loop\n",
        "\n",
        "import requests\n",
        "import bz2\n",
        "import time\n",
        "#import boto3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPRmMt8FVirP"
      },
      "source": [
        "# Start Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RedditNlpEtlPoCTest\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlx_d-6P7I6-"
      },
      "source": [
        "## Function definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L94KiWkd7NMa"
      },
      "source": [
        "### Data Retrieval and Initial Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG53pHdQ3uBv"
      },
      "source": [
        "def generate_urls(url_stub, start_year, end_year):\n",
        "  \"\"\"\n",
        "  Generates a list of URLs corresponding the the Reddit posts\n",
        "  to be retrieved. The URL pattern is somewhat idiosyncratic,\n",
        "  so if it changes this code will need to be modified.\n",
        "\n",
        "  Arguments:\n",
        "    url_stub: the common prefix used for all URLs\n",
        "    start_year, end_year: the beginning and ending years\n",
        "    of posts to retrieve. For a single year, set\n",
        "    start_year = end_year. Both start_year and end_year\n",
        "    are inclusive, so (2010, 2012) will return three years\n",
        "    of URLs\n",
        "\n",
        "  Returns:\n",
        "    list of URLs with one URL for each month and year\n",
        "    to be retrieved\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  # URLs appended to this list\n",
        "  file_urls = []\n",
        "  \n",
        "  for year in range(start_year, (end_year + 1)):\n",
        "    \n",
        "    # Set extension by year for most files\n",
        "      \n",
        "    if year < 2018:\n",
        "      extension = '.bz2'\n",
        "    elif year < 2019:\n",
        "      extension = '.xz'\n",
        "    else:\n",
        "      extension = '.zst'\n",
        "      \n",
        "    # Generate the URLs based on the observed patterns\n",
        "\n",
        "    for month in range(1,13):\n",
        "          \n",
        "      # Handle a few special cases    \n",
        "      if (year == 2017 and month == 12):\n",
        "        extension = '.xz'\n",
        "      if (year == 2018 and month in [11,12]):\n",
        "        extension = '.zst'\n",
        "\n",
        "      # Create the file name, adding the leading zero\n",
        "      # if the month is 1 - 9\n",
        "      if month < 10:\n",
        "        file = 'RC_' + str(year) + '-0' + str(month) + extension\n",
        "      else:\n",
        "        file = 'RC_' + str(year) + '-' + str(month) + extension\n",
        "              \n",
        "      file_urls.append(url_stub + file)\n",
        "\n",
        "  return file_urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ya8uwHUA_68"
      },
      "source": [
        "def get_file(url):\n",
        "  \"\"\"\n",
        "    Retrieves a file from the supplied URL using the requests library.\n",
        "    Basically a wrapper for requests.get(), but abstracted as a function\n",
        "    so type-checking and error-handling can be added if needed.\n",
        "\n",
        "    Argument:\n",
        "      url: the URL specifying the resource to be retrieved\n",
        "\n",
        "    Returns:\n",
        "      The content of the request response. In this invocation, this\n",
        "      will be the compressed file of reddit posts at the supplied URL\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  response = requests.get(url)\n",
        "\n",
        "  return response.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cchfPcRhBa1W"
      },
      "source": [
        "def unzip_file(zipped_file):\n",
        "  \"\"\"\n",
        "    Decompresses the supplied file, which must be in bzip2 format.\n",
        "    Requires the bz2 library.\n",
        "\n",
        "    Argument:\n",
        "      The compressed file in bzip2 format\n",
        "\n",
        "    Returns:\n",
        "      A (potentially very long) string containing the uncompressed\n",
        "      file contents.\n",
        "  \"\"\"\n",
        "\n",
        "  unzipped_file = bz2.decompress(zipped_file).decode()\n",
        "\n",
        "  return unzipped_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URyVd0hq7W50"
      },
      "source": [
        "### NLP Pipeline Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez9O4I8f17gG"
      },
      "source": [
        "def tokenize(nlp_df):\n",
        "    \n",
        "  nlp_tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"post_words\")\n",
        "  tokenized_df = nlp_tokenizer.transform(nlp_df)\n",
        "  return tokenized_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJGCaX02fP2"
      },
      "source": [
        "def remove_stopwords(tokenized_df):\n",
        "\n",
        "  remover = StopWordsRemover(inputCol='post_words', outputCol='post_filtered')\n",
        "  remover.loadDefaultStopWords('english')\n",
        "  filtered_df = remover.transform(tokenized_df)\n",
        "  return filtered_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57jSXMLz3ESU"
      },
      "source": [
        "def hasher(hashable_df):\n",
        "\n",
        "# Number of Features is default (262,144)\n",
        "\n",
        "  hasher = HashingTF(inputCol='post_filtered', outputCol='post_hashed')\n",
        "  hashed_df = hasher.transform(hashable_df)\n",
        "  return hashed_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPzyjjD13sR6"
      },
      "source": [
        "def tfidf_calc(hashed_df):\n",
        "\n",
        "  tfidf = IDF(inputCol='post_hashed', outputCol='post_tfidf')\n",
        "  tfidfModel = tfidf.fit(hashed_df)\n",
        "  tfidf_df = tfidfModel.transform(hashed_df)\n",
        "  return tfidf_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Is8G-X7cwt"
      },
      "source": [
        "## Main Function\n",
        "\n",
        "This function retrieves the reddit posts, decompresses them, and constructs a Spark DataFrame from them. The DataFrame is then processed by the NLP pipeline to output estimated TF-IDF scores for each post.\n",
        "\n",
        "The corpus for this pipeline is the dowloaded posts: if posts from 2006 are downloaded, then the TF-IDF scores will be relative to 2006 posts. If 2006 and 2007 are downloaded, the scores will be relative to both years; and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fohyO5yA5aMp"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Main function for prototype.\n",
        "  # Generates a list of URLs for the provided url_stub and range of years.\n",
        "  # Then uses a loop to retrieve each file, decompress it,\n",
        "  # and generate an RDD.\n",
        "  # The union of these RDDs generates the DataFrame,\n",
        "  # and the NLP pipeline outputs TF-IDF scores for each post\n",
        "  # to a new column.\n",
        "\n",
        "  start_year = 2006\n",
        "  end_year = 2006\n",
        "  url_stub = 'https://files.pushshift.io/reddit/comments/'\n",
        "  #s3_bucket_prefix = 'reddit-nlp-etl-spark-posts'\n",
        "  reddit_urls = generate_urls(url_stub, start_year, end_year)\n",
        "\n",
        "  sc = spark.sparkContext\n",
        "  #s3 = boto3.resource('s3')\n",
        "\n",
        "  rdd_list = []\n",
        "\n",
        "  # Display starting time (current time)\n",
        "  print(f'Main loop starting at', {time.asctime()}, '\\n')\n",
        "  for url in reddit_urls:\n",
        "\n",
        "    # Get file from website and decompress it to text  \n",
        "    file_name = url.split('/')[5].split('.')[0]\n",
        "    print(f'Now processing', file_name)\n",
        "\n",
        "    reddit_file = get_file(url)\n",
        "    unzipped_file = unzip_file(reddit_file)\n",
        "  \n",
        "    # Not saving files during this test run\n",
        "    # s3.Bucket(s3_bucket_prefix).put_object(Key=file_name, Body=unzipped_file)\n",
        "\n",
        "    # Make an RDD from the file and append it to a list\n",
        "    print('Generating RDD from file and appending to rdd_list\\n')\n",
        "    rdd_list.append(sc.parallelize(unzipped_file.splitlines()))\n",
        "\n",
        "  \n",
        "  # Create Spark DataFrame from RDDs\n",
        "  # (easier than appending each one in succession;\n",
        "  # not sure if faster or not)\n",
        "  print(f'Creating DataFrame of posts from years', start_year, 'to', end_year, '\\n')\n",
        "  reddit_df = spark.read.json(sc.union(rdd_list))\n",
        "\n",
        "  # NLP pipeline\n",
        "  print('Beginning NLP pipeline')\n",
        "  print(f'\\t* Tokenizing posts')\n",
        "  reddit_tokenized_df = tokenize(reddit_df)\n",
        "  print(f'\\t* Removing stop words')\n",
        "  reddit_filtered_df = remove_stopwords(reddit_tokenized_df)\n",
        "  print(f'\\t* Hashing posts')\n",
        "  reddit_hashed_df = hasher(reddit_filtered_df)\n",
        "  print(f'\\t* Calculating TF-IDF scores')\n",
        "  reddit_tfidf_df = tfidf_calc(reddit_hashed_df)\n",
        "\n",
        "  # Display concluding time\n",
        "  print(f'\\nScript concluded at', {time.asctime()})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxCWVnM5P7sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7384c4-5e3f-44f0-a0f6-bb8861add27b"
      },
      "source": [
        "# Check a few rows to make sure everything worked\n",
        "\n",
        "cols = ['author', 'body', 'post_words', 'post_filtered', 'post_hashed', 'post_tfidf']\n",
        "reddit_tfidf_df.select(cols).show(5, truncate=False, vertical=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " author        | jh99                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
            " body          | early 2006 a probable date                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            " post_words    | [early, 2006, a, probable, date]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            " post_filtered | [early, 2006, probable, date]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            " post_hashed   | (262144,[90957,133480,168756,187436],[1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
            " post_tfidf    | (262144,[90957,133480,168756,187436],[6.738749530295565,6.57481459975205,8.021304121655362,5.597212196910421])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " author        | jpb                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            " body          | If you are going to post something that has a link to the original author, why not just post the original instead of someone's copy?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
            " post_words    | [if, you, are, going, to, post, something, that, has, a, link, to, the, original, author,, why, not, just, post, the, original, instead, of, someone's, copy?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            " post_filtered | [going, post, something, link, original, author,, post, original, instead, someone's, copy?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
            " post_hashed   | (262144,[52351,57058,62145,68106,71092,102382,133774,168138,172933],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            " post_tfidf    | (262144,[52351,57058,62145,68106,71092,102382,133774,168138,172933],[4.4080216758897555,3.350592268214216,9.945552773929496,4.560599100721913,7.937338741538029,3.465124849912659,10.014976345336153,6.44601949154648,9.000650324133678])                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " author        | Pichu0102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            " body          | Microsoft hates it's own products?\r\n",
            "Who knew?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            " post_words    | [microsoft, hates, it's, own, products?, , who, knew?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            " post_filtered | [microsoft, hates, products?, , knew?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            " post_hashed   | (262144,[7987,66118,146088,164154,249180],[1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            " post_tfidf    | (262144,[7987,66118,146088,164154,249180],[7.0524070891506065,9.805790831554338,9.850242594125172,5.7107218940741955,0.8882120777732349])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " author        | libertas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
            " body          | this looks interesting, but it's already aired, and it's not like there's streaming video, so what's the point?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            " post_words    | [this, looks, interesting,, but, it's, already, aired,, and, it's, not, like, there's, streaming, video,, so, what's, the, point?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
            " post_filtered | [looks, interesting,, already, aired,, like, streaming, video,, point?]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
            " post_hashed   | (262144,[2306,17153,101619,112733,126677,174523,208258,225792],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            " post_tfidf    | (262144,[2306,17153,101619,112733,126677,174523,208258,225792],[4.27051276813895,6.363423689762441,8.066087724282337,4.564043816494696,7.063549265703848,7.060752061082788,2.2691950359728086,10.302227717868229])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
            "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            " author        | mdmurray                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
            " body          | I have nothing but good things to say about Dell Tech Support. Many a time I've called in a faulty part and had the replacement at the front door in two days with a box to ship the old one back. First class service if you ask me.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            " post_words    | [i, have, nothing, but, good, things, to, say, about, dell, tech, support., many, a, time, i've, called, in, a, faulty, part, and, had, the, replacement, at, the, front, door, in, two, days, with, a, box, to, ship, the, old, one, back., first, class, service, if, you, ask, me.]                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            " post_filtered | [nothing, good, things, say, dell, tech, support., many, time, called, faulty, part, replacement, front, door, two, days, box, ship, old, one, back., first, class, service, ask, me.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            " post_hashed   | (262144,[8804,21823,22370,24698,26143,43756,62606,77715,84104,90723,103497,113432,116996,117484,121517,127310,130809,171222,182235,188835,195155,214676,218360,233903,235415,245044,258728],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                             \n",
            " post_tfidf    | (262144,[8804,21823,22370,24698,26143,43756,62606,77715,84104,90723,103497,113432,116996,117484,121517,127310,130809,171222,182235,188835,195155,214676,218360,233903,235415,245044,258728],[4.128888142216623,2.510704898717497,6.327900829103928,5.215513605895536,7.357788738701789,5.594629884306949,6.988041713195703,7.08621312528106,8.249937165254345,4.984808249446706,4.478125744559736,2.9953128833412044,4.0924885965575415,4.626942704113698,3.2815902818294194,6.176246070702946,7.642967680935451,3.388135417430211,4.382374262715381,3.62694508548882,6.370402085143903,3.599916413100901,5.186803500013105,6.590399330768748,5.392202336671201,3.289597755511374,3.8314282140856273]) \n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHH8_TqOCE8J",
        "outputId": "39ee95b3-8ba7-45de-c574-6fa65660bee5"
      },
      "source": [
        "# Print schema\n",
        "\n",
        "reddit_tfidf_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- author_flair_css_class: string (nullable = true)\n",
            " |-- author_flair_text: string (nullable = true)\n",
            " |-- body: string (nullable = true)\n",
            " |-- controversiality: long (nullable = true)\n",
            " |-- created_utc: long (nullable = true)\n",
            " |-- distinguished: string (nullable = true)\n",
            " |-- edited: string (nullable = true)\n",
            " |-- gilded: long (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- link_id: string (nullable = true)\n",
            " |-- parent_id: string (nullable = true)\n",
            " |-- retrieved_on: long (nullable = true)\n",
            " |-- score: long (nullable = true)\n",
            " |-- stickied: boolean (nullable = true)\n",
            " |-- subreddit: string (nullable = true)\n",
            " |-- subreddit_id: string (nullable = true)\n",
            " |-- ups: long (nullable = true)\n",
            " |-- post_words: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- post_filtered: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- post_hashed: vector (nullable = true)\n",
            " |-- post_tfidf: vector (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgcRiIgTDFaR"
      },
      "source": [
        "# Drop unneeded fields\n",
        "\n",
        "reddit_export_df = reddit_tfidf_df.drop(\n",
        "    'author_flair_css_class', 'author_flair_text', 'body', 'created_utc', 'edited', 'link_id', 'parent_id', 'retrieved_on', 'subreddit_id', 'post_words', 'post_filtered', 'post_hashed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYj0JCdlEK5S",
        "outputId": "b0f7fc6e-03f1-473e-b470-6bc62b609ced"
      },
      "source": [
        "reddit_export_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----------------+-------------+------+-----+-----+--------+----------+---+--------------------+\n",
            "|          author|controversiality|distinguished|gilded|   id|score|stickied| subreddit|ups|          post_tfidf|\n",
            "+----------------+----------------+-------------+------+-----+-----+--------+----------+---+--------------------+\n",
            "|            jh99|               0|         null|     0|c2715|    0|   false|reddit.com|  0|(262144,[90957,13...|\n",
            "|             jpb|               0|         null|     0|c2717|    0|   false|reddit.com|  0|(262144,[52351,57...|\n",
            "|       Pichu0102|               0|         null|     0|c2718|    2|   false|reddit.com|  2|(262144,[7987,661...|\n",
            "|        libertas|               0|         null|     0|c2719|    2|   false|reddit.com|  2|(262144,[2306,171...|\n",
            "|        mdmurray|               0|         null|     0|c2722|    0|   false|reddit.com|  0|(262144,[8804,218...|\n",
            "|        mdmurray|               0|         null|     0|c2723|    1|   false|reddit.com|  1|(262144,[20737,50...|\n",
            "|        mattknox|               0|         null|     0|c2724|    2|   false|reddit.com|  2|(262144,[528,991,...|\n",
            "|       [deleted]|               0|         null|     0|c2725|    1|   false|reddit.com|  1|(262144,[97284],[...|\n",
            "|           sempf|               0|         null|     0|c2726|    6|   false|reddit.com|  6|(262144,[4390,218...|\n",
            "|          dayuii|               0|         null|     0|c2727|    1|   false|reddit.com|  1|(262144,[198197],...|\n",
            "|         dfranke|               0|         null|     0|c2728|    3|   false|reddit.com|  3|(262144,[66927,14...|\n",
            "|           jcage|               0|         null|     0|c2729|    2|   false|reddit.com|  2|(262144,[11018,11...|\n",
            "|TheCookieMonster|               0|         null|     0|c2730|    1|   false|reddit.com|  1|(262144,[32983,35...|\n",
            "|           scott|               0|         null|     0|c2731|    1|   false|reddit.com|  1|(262144,[212976],...|\n",
            "|          stesch|               0|         null|     0|c2732|    2|   false|reddit.com|  2|(262144,[422,7095...|\n",
            "|       [deleted]|               0|         null|     0|c2733|   -7|   false|reddit.com| -7|(262144,[97284],[...|\n",
            "|    lupin_sansei|               0|         null|     0|c2734|    1|   false|reddit.com|  1|(262144,[70470,83...|\n",
            "|          savvy0|               0|         null|     0|c2735|    1|   false|reddit.com|  1|(262144,[5303,261...|\n",
            "|       [deleted]|               0|         null|     0|c2736|   -1|   false|reddit.com| -1|(262144,[97284],[...|\n",
            "|       satsumace|               0|         null|     0|c2737|    3|   false|reddit.com|  3|(262144,[1696,785...|\n",
            "+----------------+----------------+-------------+------+-----+-----+--------+----------+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}